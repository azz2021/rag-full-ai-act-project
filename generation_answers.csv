id,query,answer
q01,What are the AI risk categories defined by the EU AI Act?,"[C2] (chunk_id=eu_38_0, doc=EU_AI_Act, page=-1) Some AI systems are considered 'High risk' under the AI Act. Providers of those systems will be subject to additional requirements. [C3] (chunk_id=eu_0_0, doc=EU_AI_Act, page=-1) High-level summary of the AI Act [C4] (chunk_id=eu_2_0, doc=EU_AI_Act, page=-1) In this article we provide you with a high-level summary of the AI Act, selecting the parts which are most likely to be relevant to you rega [C5] (chunk_id=eu_10_0, doc=EU_AI_Act, page=-1) Those that intend to place on the market or put into service high-risk AI systems in the EU, regardless of whether they are based in the EU"
q02,List the prohibited AI practices under the EU AI Act.,"[C1], [C2]"
q03,Name three core obligations for providers of high-risk AI systems.,"[C3] (chunk_id=eu_49_0, doc=EU_AI_Act, page=-1) Requirements for providers of high-risk AI systems (Art. 8-17) [C4] (chunk_id=eu_38_0, doc=EU_AI_Act, page=-1) Some AI systems are considered 'High risk' under the AI Act. Providers of those systems will be subject to additional requirements. [C5] (chunk_id=eu_13_0, doc=EU_AI_Act, page=-1)"
q04,When do the obligations for high-risk AI systems start to apply?,"24 months for high risk AI systems under Annex III. [C3] (chunk_id=eu_49_0, doc=EU_AI_Act, page=-1) Requirements for providers of high-risk AI systems (Art. 8-17) [C4] (chunk_id=eu_38_0, doc=EU_AI_Act, page=-1) Some AI systems are considered 'High risk' under the AI Act. Providers of those systems will be subject to additional requirements. [C5] (chunk_id=eu_51_0, doc=EU_AI_Act, page=-1) Establish a risk management system throughout the high risk AI system's lifecycle;"
q05,What qualifies a GPAI model as creating systemic risk under the Act?,when the cumulative amount of compute used for its training is greater than 1025 floating point operation
q06,Why does the Transformer remove recurrence and convolution?,"reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with [C4] (chunk_id=tr_4_0, doc=Transformer, page=4) output values."
q07,Explain scaled dot-product attention.,"[C1] (chunk_id=tr_3_0, doc=Transformer, page=3) Scaled Dot-Product Attention Multi-Head Attention consists of se [C2] (chunk_id=tr_12_0, doc=Transformer, page=12) Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 mak [C3] (chunk_id=tr_13_0, doc=Transformer, page=13) Input-Input Layer5 The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . EOS> [C4] (chunk_id=tr_4_0, doc=Transformer, page=4) output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention [C5] (chunk_id=tr_9_1, doc"
q08,What is multi-head attention and why is it useful?,"Multi-head attention [C2] (chunk_id=tr_12_0, doc=Transformer, page=12) Attention Visualizations Input-Input Layer5 It is in this spirit that a majority of American governments have passed new laws since 2009 mak [C3] (chunk_id=tr_3_0, doc=Transformer, page=3) Scaled Dot-Product Attention Multi-Head Attention"
q09,How do sinusoidal positional encodings work?,"[C1] (chunk_id=tr_4_1, doc=Transformer, page=4) to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN( x) = max(0"
q10,"State the base modelâ€™s typical hyperparameters (e.g., d_model, heads).","[C3] (chunk_id=tr_4_1, doc=Transformer, page=4) output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention [C4] (chunk_id=tr_6_1, doc=Transformer, page=4) output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention [C5] (chunk_id=tr_5_0, doc=Transformer, page=5) output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention [C4] (chunk_id=tr_6_1, doc=Transformer, page=4) output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention [C5] (chunk_id=tr_5_0, "
